- “the first sequence transduction model based entirely on **attention**”
- **sequence-to-sequence** [https://wikidocs.net/24996]
    - Encoder, Decoder 구조를 갖는다.
    - encoder, decoder는 각각 여러개의 RNN 셀로 구성이 되어있다.
    - <img src = "https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG">
    - Encoder에서 각각의 단어에 대해 embedding된 결과를 입력으로 받아 Encoder의 RNN Structure를 통과한 후, 마지막 cell의 hidden state를 Decoder의 첫번째 cell의 hidden state input으로 넣어주는데 이를 __context vector__라고 한다.
    - Decoder에서는 각 셀의 output gate에서 나온 결과에 softmax함수를 적용해 모델이 가지고 있는 단어 후보군으로부터 어떤 단어를 출력할지 계산하고 결정하게 된다.
    - <img src = "https://wikidocs.net/images/page/24996/decodernextwordprediction.PNG">
    - 하나의 모델에서입력 시퀀스와 출력 시퀀스의 개수가 다를 수 있다.
    - 입력 시퀀스의 도메인과 출력 시퀀스의 도메인이 다를 수 있다.
    - e.g. French → English
- Attention
    - seq2seq의 단점   
        - seq2seq은 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하기 때문에 정보 손실 발생
        - RNN모델의 문제인 Vanishing Gradient
    - __Decoder에서 출력 단어를 예측하는 매시점마다 인코더에서의 전체 입력 문장을 다시 한번 참고하게 된다.__
    - Query, Key, Value
        - Attention(Q,K,V) = Attention Value
        - Attention함수는 주어진 Query에 대해서 모든 Key와의 유사도를 각각 구하고, 이 유사도를 Value에 반영해주고 그 Value vector를 모두 더해서 return
        - __Query__ : t 시점의 디코더 셀에서의 hidden state = decoder의 현시점(t)에서의 Hidden state
        - __Key__ : 모든 시점의 인코더 셀의 hidden state(mapping) = 
        - __Value__ : 모든 시점의 인코더 셀의 hidden state(mapping)
        - Key, Value 는 Dictionary에서 사용하는 개념과 같다.
    - Dot product Attention
        - ![image](https://user-images.githubusercontent.com/81205952/194202905-b1050cbd-bd0f-4ce8-9ae9-0a4cd2ee5f94.png)

- Transformer의 기본 원리 [https://wikidocs.net/31379]
    - __IDEA__ : RNN구조를 따로 사용하지 않고, attention(encoder, decoder)만으로 모델을 구성
    - Encoder
        - Multi-head Self-attention + Feed forward Neural Network(FFNN)
        - Self-attention
            - Encoder내에서만 attention 수행
            - Query,Key,Value가 모두 Encoder에서 생성
            - input을 vector로 encoding할 때, 각각의 $x$만을 고려하는 것이 아니라, 다른 $x_i$들도 고려하게 된다.
            - Q,K,V를 얻는 방법
                - n개의 단어를 $d_{model}$의 차원 vector로 embedding 한 후에, $d_{model} * (d_{model}/num\ heads)$의 크기를 갖는 가중치 행렬과의 dot product연산으로 각각의 Q,K,V 벡터를 얻는다
            - Embedding vector 생성 후,
                1. Query , Key, Value vector를 embedding vector로 부터 만들어냄
                2. 그 후, query와 나머지 다른 단어들의 key vector의 내적으로 score를 계산
                3. 그 값을 sqrt(key vector’s dim)으로 나누고 softmax 함수에 대입 → # value vector의 weight
                4. 그 결과를 value 벡터에 곱해주고 sum을 취한 값을 사용 → **weighted sum of ‘value vector’**
        - Transformer의 neural network는 가변적이고 유연한 모델이다 → 성능이 좋아짐
        - Multi headed attention(MHA) : Attention을 여러번 반복
            - encoding 결과가 n개 나오게 된다
        - Positional encoding
            - self-attention 연산은 input의 순서를 고려하지 않는다
            - 따라서, 주어진 입력에 어떤 값을 더해준다
        - Self Attention 후에는 feed forward 연산을 수행한다
    - Decoder
        - key, value를 encoder에서 decoder로 보낸다 → Encoder-Decoder Attention
        - output 결과는 순서대로 나옴
        - Masked Decoder Attention
            - 이전 단어들에 대해서만 dependent, 이후 단어들에 대해서는 independent하게 학습
- 다른 분야에 적용된 Transformer 기법
    - Vision Transformer
        - 이미지 데이터에 대해서 transformer 사용
    - DALL-E
        - 문장이 주어졌을 때, 이미지를 생성
- Embedding과 Encoding의 차이
    - [https://jamesmccaffrey.wordpress.com/2022/08/02/the-difference-between-encoding-embedding-and-latent-representation-in-my-world/]
    - Embedding
        - Embedding converts an integer word ID to a vector
        - e.g. “the” = 4  = [-0.1234, 1.9876,…, 3.4681]
    - Encoding
        - Encoding converts categorical data to numeric data
        - e.g. “red” = [ 0 1 0 0]
